---
title: "Data cleaning and formatting"
output: html_notebook
---


# Data sources

This project needs data on:

  * crime from [CODE](https://osf.io/zyaqn/),
  * schools from the National Center for Education Statistics [Common Core of 
    Data](https://nces.ed.gov/ccd/pubschuniv.asp) (CCD).

The output from this file should be a table in which each row represents a 
census block group that is within a CODE city and has a population greater than 
zero. Each case should include counts of crime in the area as well as counts of 
schools and school students of different ages. Spatially lagged counts of crime 
are also required.


# Pre-processing

No pre-processing was requried for the data for this project. All processing of 
data before analysis is done in this file.


# Set up basic variables

```{r}
# Create vector of counties containing CODE cities
# Note that Kansas City and New York City are in multiple counties
code_counties <- c(
	"17031", # Cook County, IL
	"26163", # Wayne County, MI
	"48121", # Denton County, TX
	"48439", # Tarrant County, TX
	"29095", # Jackson County, KY
	"29047", # Clay County, KY
	"29037", # Cass County, KY
	"29165", # Platte County, KY
	"06037", # Los Angeles County, CA
	"21111", # Jefferson County, KY
	"36005", # Bronx County, NY
	"36047", # Kings County, NY
	"36061", # New York County, NY
	"36081", # Queens County, NY
	"36085", # Richmond County, NY
	"06075", # San Francisco County, CA
	"04019", # Pima County, AZ
	"51810" # Virginia Beach County, VA
)
# Create list of identifiers for CODE cities
cities <- tribble(
  ~name, ~fips, ~prefix, ~state,
  "Chicago",        "17", "chi", "IL",
  "Detroit",        "26", "dtt", "MI",
  "Fort Worth",     "48", "ftw", "TX",
  "Kansas City",    "29", "kcm", "MO",
  "Los Angeles",    "06", "lax", "CA",
  "Louisville",     "21", "lou", "KY",
  "New York",       "36", "nyc", "NY",
  "San Francisco",  "06", "sfo", "CA",
  "Tucson",         "04", "tus", "AZ",
  "Virginia Beach", "51", "vib", "VA"
)
```


# Neighbourhood data


## Geographic boundary files

```{r}
if (!file.exists("../original_data/block_groups.gpkg")) {
	
	# set cache folder
	tigris_cache_dir("original_data")
	readRenviron('~/.Renviron')
	
	# download block group outlines for counties containing CODE cities
	blockgroup_outlines <- lapply(code_counties, function (x) {
		# block_groups() from tigris
		block_groups(
			state = str_sub(x, 1, 2),
			county = str_sub(x, 3, 5),
			cb = FALSE,
			year = 2016,
			class = "sf"
		)
	}) %>% 
		reduce(rbind) %>%
		st_transform(4326) %>% 
		select(
			geoid = GEOID,
			state = STATEFP,
			county = COUNTYFP,
			census_tract = TRACTCE,
			census_block_group = BLKGRPCE,
			area_land = ALAND, # in square metres
			area_water = AWATER # in square metres
		)
	
	# save a copy of the block groups data
	st_write(blockgroup_outlines, "../original_data/block_groups.gpkg", 
					 quiet = TRUE)
	
} else {
	
	# load block group outlines
	blockgroup_outlines <- st_read("../original_data/block_groups.gpkg", 
																 stringsAsFactors = FALSE)
	
}

glimpse(blockgroup_outlines)

if (!file.exists("../original_data/city_outlines.gpkg")) {
	
	# download city outlines
	code_city_outlines <- lapply(cities$name, function (x) {
		# places() from tigris
		places(
			state = cities$fips[cities$name == x],
			cb = FALSE,
			year = 2016,
			class = "sf"
		) %>% 
			filter(NAME == x)
	}) %>% 
		reduce(rbind) %>% 
		st_transform(4326) %>% 
		select(
			state = STATEFP,
			geoid = GEOID,
			city_name = NAME,
			area_land = ALAND,
			area_water = AWATER
		)
	
	# remove the Farallon Islands from the outline of San Francisco
	code_city_outlines <- code_city_outlines %>% 
		st_cast("POLYGON") %>% 
		add_centroid_coordinates() %>%
		filter(X > -123) %>% 
		select(-X, -Y) %>% 
		group_by(city_name) %>%
		summarise(state = first(state))
	
	# save a copy of the city outline data
	st_write(code_city_outlines, "../original_data/city_outlines.gpkg", 
					 quiet = TRUE, layer_options = c("OVERWRITE=YES"))
	
} else {
	
	# read city outlines
	code_city_outlines <- st_read("../original_data/city_outlines.gpkg", 
																stringsAsFactors = FALSE)
	
}

glimpse(code_city_outlines)

# filter out block groups that are not at least partly within a CODE city
blockgroup_outlines <- blockgroup_outlines %>% 
	st_join(select(code_city_outlines, -state)) %>% 
	filter(!is.na(city_name))

# remove any block groups that are mostly outside city boundaries
blockgroup_outlines <- blockgroup_outlines %>% 
	add_overlap_area(code_city_outlines) %>% 
	filter(prop_overlap > 0.5) %>% 
	select(-prop_overlap) %>% 
	glimpse()

# create list-column of blockgroups that intersect (i.e. have queen contiguity 
# with) each blockgroup
# This is done by calling st_intersects() with only one argument, which 
# intersects the layer with itself. The map function is needed because 
# st_intersects() returns a sparse geometry binary predicate list (SGBP) which
# is similar to a matrix and so cannot be stored in a tibble (map returns a 
# normal list). This works because intersecting the layer with itself produces
# SGBPs each with only one dimension.
blockgroup_outlines$adj <- st_intersects(blockgroup_outlines) %>% 
	map(function (x) x)

# the adjacency list consists of row numbers, which will not work with long-
# format crime in which each blockgroup is represented by multiple rows. To deal
# with this, we can create a corresponding list column of geoids
blockgroup_outlines$adj_geoid <- map(1:nrow(blockgroup_outlines), function (i) {
	blockgroup_outlines$geoid[blockgroup_outlines$adj[[i]]]
})
```


## Land cover data

Land cover data for census block groups are available from [Land cover estimates 
for census geographies](https://osf.io/p4xus/). This project includes a file for 
each state showing the proportion of each block group with each type of land 
cover. These can be matched to the other data using the block group GEOID.

We only need to know what proportion of the land in each blockgroup is developed 
but not open space, i.e. in [NLCD categories](https://www.mrlc.gov/nlcd11_leg.php)
22, 23 and 24.

```{r}
land_cover_data <- dir("../original_data", pattern = "^land_cover_", 
											 full.names = TRUE) %>% 
	map(function (x) {
		read_csv(x, col_types = cols(
			blockgroup = col_character(),
			.default = col_number()
		)) %>% 
			mutate(county = str_sub(blockgroup, 0, 5)) %>% 
			filter(county %in% code_counties) %>% 
			select(-county)
	}) %>% 
	bind_rows() %>% 
	rename_at(vars(-blockgroup), ~ paste0("land_cover_", .)) %>% 
	mutate(prop_developed = land_cover_prop_22 + land_cover_prop_23 + 
				 	land_cover_prop_24) %>% 
	select(blockgroup, prop_developed, land_cover_prop_22, land_cover_prop_23, 
				 land_cover_prop_24) %>% 
	mutate(
		prop_developed = ifelse(prop_developed > 1, 1, prop_developed)
	) %>% 
	glimpse()
```

```{r}
blockgroup_outlines <- blockgroup_outlines %>% 
	left_join(land_cover_data, by = c("geoid" = "blockgroup")) %>% 
	glimpse()

rm(land_cover_data)
```

## Population data

Population data are needed to account for the different nature of different 
census blocks. The variables that will be included in the model come from 
[Haberman and Ratcliffe (2015)](http://doi.org/10.1111/1745-9125.12076), with 
the addition of a variable showing the proportion of the population who are 
teenagers.

Scaling/centering variables can help to ease interpretation, particularly of the
intercept term. `glmer.nb()` also struggles to converge when some variables are
on very different scales to others. To deal with this, we will scale and centre 
some predictors. Variables with a `_sc` suffix have been mean-centred and scaled 
using `scale()`.

```{r}
if (!file.exists("../original_data/acs_data.csv")) {
	
	# download 5-year ACS estimates at block-group level
	acs_data <- map_df(code_counties, function (x) {
		# get_acs() from tidycensus
		get_acs(
			geography = "block group",
			variables = c(
				pop_total = "B01001_001E",
				gpop_male_10_14 = "B01001_005E",
				gpop_male_15_17 = "B01001_006E",
				gpop_male_18_19 = "B01001_007E",
				gpop_feml_10_14 = "B01001_029E",
				gpop_feml_15_17 = "B01001_030E",
				gpop_feml_18_19 = "B01001_031E",
				eth_white = "B03002_003E",
				eth_black = "B03002_004E",
				eth_hispanic = "B03002_012E",
				eth_asian = "B03002_006E",
				eth_not_hispanic = "B03002_002E",
				move_same = "B07201_002E",
				edu_total = "B15003_001E",
				gedu_highs = "B15003_017E",
				gedu_gedal = "B15003_018E",
				gedu_some1 = "B15003_019E",
				gedu_somem = "B15003_020E",
				gedu_assoc = "B15003_021E",
				gedu_batch = "B15003_022E",
				gedu_mastr = "B15003_023E",
				gedu_profl = "B15003_024E",
				gedu_doctr = "B15003_025E",
				pov_total = "B17010_001E",
				pov_inpov = "B17010_002E",
				pov_nopov = "B17010_022E",
				median_income = "B19013_001E",
				occ_total = "B25003_001E",
				occ_renting  = "B25003_003E"
			),
			year = 2016, # this is the end year of the 5-year period
			output = "wide",
			state = as.integer(str_sub(x, 1, 2)),
			county = as.integer(str_sub(x, 3)),
			key = census_api_key
		)
	}, .id = "county")
	
	# save a copy of the raw ACS data to a file
	write_csv(acs_data, "../original_data/acs_data.csv", na = "")
	
	# clean up
	rm(census_api_key)
	
} else {
	
	# load ACS data
	acs_data <- read_csv("../original_data/acs_data.csv")
	
}

# sum variables where required
acs_data <- acs_data %>% mutate(
	eth_other = eth_not_hispanic - (eth_white + eth_black + eth_asian),
	pop_teen = rowSums(select(., starts_with("gpop_"))),
	edu_school_grad = rowSums(select(., starts_with("gedu_")))
) %>% select(
	-eth_not_hispanic,
	-starts_with("gpop_"),
	-starts_with("gedu_"),
	-ends_with("M"), # remove all columns containing margins of error
	-GEOID1, -NAME1 # remove duplicate name and GEOID columns at end of data
)

# convert ACS variables to variables needed for analysis
acs_data <- acs_data %>% 
	mutate(
		# percentage of residents over 25 without a high-school degree
		perc_no_degree = 1 - (edu_school_grad / edu_total),
		# percentage of families in poverty
		perc_poverty = pov_inpov / pov_total,
		# percentage of renter-occupied housing units
		perc_renters = occ_renting / occ_total,
		# percentage of residents who moved in the past year
		perc_moved = 1 - (move_same / pop_total),
		# percentage of residents in each ethnic group,
		perc_eth_white = eth_white / pop_total,
		perc_eth_black = eth_black / pop_total,
		perc_eth_hispanic = eth_hispanic / pop_total,
		perc_eth_asian = eth_asian / pop_total,
		perc_eth_other = eth_other / pop_total,
		# percentage of residents who are teenagers
		perc_teen = pop_teen / pop_total
	) %>% 
	# since some block groups have zero population, some of the above calculations
	# can produce NA values, which then result in *all* block groups having the
	# value NA once scale() is called, below. We will therefore filter out these
	# cases now
	filter_at(vars(starts_with("perc_"), median_income), all_vars(!is.na(.))) %>% 
	select(geoid = GEOID, name = NAME, pop_total, perc_no_degree, perc_poverty,
				 perc_renters, perc_moved, perc_eth_asian, perc_eth_black, 
				 perc_eth_hispanic, perc_eth_other, perc_eth_white, perc_teen,
				 median_income) %>% 
	glimpse()

# scale and center variables
# acs_data <- acs_data %>% 
# 	mutate(
# 		perc_no_degree_sc = scale_vector(perc_no_degree),
# 		perc_poverty_sc = scale_vector(perc_poverty),
# 		median_income_sc = scale_vector(median_income) * -1, # reverse scaled
# 		perc_renters_sc = scale_vector(perc_renters),
# 		perc_moved_sc = scale_vector(perc_moved),
# 		perc_teen_sc = scale_vector(perc_teen),
# 		pop_total_sc = scale_vector(pop_total)
# 	)

# calculate indices and remove unnecessary variables
# acs_data <- acs_data %>% 
# 	mutate(
# 		# index of concentrated disadvantage
# 		index_disadvantage = (perc_no_degree_sc + perc_poverty_sc + 
# 														median_income_sc) / 3,
# 		# index of residential mobility
# 		index_mobility = (perc_renters_sc + perc_moved_sc) / 2,
# 		# index of ethnic heterogeneity
# 		index_ethnic = 1 - (perc_eth_white^2 + perc_eth_black^2 + 
# 													perc_eth_hispanic^2 + perc_eth_asian^2 + 
# 													perc_eth_other^2),
# 	) %>% 
# 	select(geoid = GEOID, name = NAME, pop_total, starts_with("index_"),
# 				 perc_teen_sc, pop_total_sc) %>% 
# 	glimpse()

```


```{r}
# Join ACS data to blockgroup outlines to remove data from those that fall
# inside counties with CODE cities but outside the cities themselves. This is
# needed before scaling the variables and calculating indices.
blockgroup_outlines <- blockgroup_outlines %>% 
	left_join(mutate(acs_data, geoid = str_pad(geoid, 12, pad = "0")), 
						by = "geoid") %>% 
	select(-name) %>% 
	mutate_at(
		c("perc_no_degree", "perc_poverty", "median_income", "perc_renters", 
			"perc_moved", "perc_teen", "pop_total"),
		funs(ifelse(is.na(.), 0, .))
	) %>% 
	glimpse()

# scale and center ACS variables, removing the unscaled variables other than
# total population
blockgroup_outlines <- blockgroup_outlines %>% 
	group_by(city_name) %>% 
	mutate(
		perc_no_degree_sc = scale_vector(perc_no_degree),
		perc_poverty_sc = scale_vector(perc_poverty),
		median_income_sc = scale_vector(median_income) * -1, # reverse scaled
		perc_renters_sc = scale_vector(perc_renters),
		perc_moved_sc = scale_vector(perc_moved),
		perc_teen_sc = scale_vector(perc_teen),
		pop_total_sc = scale_vector(pop_total)
	) %>% 
	ungroup()

# calculate indices and remove unnecessary variables
blockgroup_outlines <- blockgroup_outlines %>% 
	mutate(
		# index of concentrated disadvantage
		index_disadvantage = (perc_no_degree_sc + perc_poverty_sc + 
														median_income_sc) / 3,
		# index of residential mobility
		index_mobility = (perc_renters_sc + perc_moved_sc) / 2,
		# index of ethnic heterogeneity
		index_ethnic = 1 - (perc_eth_white^2 + perc_eth_black^2 + 
													perc_eth_hispanic^2 + perc_eth_asian^2 + 
													perc_eth_other^2),
	) %>% 
	select(-perc_no_degree, -perc_poverty, -median_income, -perc_renters, 
				 -perc_moved, -perc_teen, -perc_eth_white, -perc_eth_black, 
				 -perc_eth_hispanic, -perc_eth_asian, -perc_eth_other,
				 -perc_no_degree_sc, -perc_poverty_sc, -median_income_sc, 
				 -perc_renters_sc, -perc_moved_sc, -perc_teen_sc) %>% 
	glimpse()

rm(acs_data)
```


## Zoning data

We can download zoning data from city websites (using cached versions if
appropriate) and then convert local codes to consistent high-level types.

```{r}
zoning <- list()

# Chicago
download_if_needed(
	remote = "https://data.cityofchicago.org/api/geospatial/7cve-jgbp?method=export&format=GeoJSON",
	local = "../original_data/zoning_chicago.geojson"
)
zoning$chi <- st_read("../original_data/zoning_chicago.geojson") %>% 
	mutate(
		zone_code = str_sub(zone_class, 0, 2),
		zone_code_main = str_sub(zone_class, 0, 1),
		land_use = case_when(
			zone_code == "DR" ~ "residential",
			zone_code == "PM" ~ "commercial",
			zone_code == "PO" ~ "public open space",
			zone_code_main %in% c("B", "C", "M") ~ "commercial",
			zone_code_main == "D" ~ "downtown",
			zone_code_main == "R" ~ "residential",
			TRUE ~ "other"
		)
	) %>% 
	select(-zone_code, -zone_code_main)

# Detroit
download_if_needed(
	remote = "https://data.detroitmi.gov/api/geospatial/62rv-kyp9?method=export&format=GeoJSON",
	local = "../original_data/zoning_detroit.geojson"
)
zoning$dtt <- st_read("../original_data/zoning_detroit.geojson") %>% 
	mutate(
		zone_code = str_sub(zcode_n, 0, 1),
		land_use = case_when(
			zcode_n == "750" ~ "commercial",
			zone_code == "1" ~ "residential",
			zone_code %in% c("2", "3", "5", "6") ~ "commercial",
			zone_code == "4" ~ "downtown",
			zone_code == "8" ~ "public open space",
			TRUE ~ "other"
		)
	) %>% 
	select(-zone_code)

# Fort Worth
download_if_needed(
	remote = "http://mapit.fortworthtexas.gov/giszipfiles/ADM_ZONING.zip",
	local = "../original_data/zoning_fort_worth.zip"
)
if (!dir.exists("../original_data/zoning_fort_worth/")) {
	unzip(zipfile = "../original_data/zoning_fort_worth.zip", 
				exdir = "../original_data/zoning_fort_worth/")
}
zoning$ftw <- st_read("../original_data/zoning_fort_worth/ADM_ZONING.shp") %>% 
	mutate(
		zone_code = str_extract(ZONING, "^\\w+"),
		land_use = case_when(
			zone_code %in% c("E", "ER", "F", "FR", "G", "I", "J", "K") ~ "commercial",
			zone_code == "H" ~ "downtown",
			zone_code %in% c("A", "AR", "B", "C", "CR", "D", "MH", "R1", "R2", "UR") ~ 
				"residential",
			TRUE ~ "other"
		)
	) %>% 
	st_transform(4326)

# Kansas City
download_if_needed(
	remote = "http://maps.kcmo.org/apps/download/GisDataDownload/Other.gdb.zip",
	local = "../original_data/zoning_kansas_city.zip"
)
if (!dir.exists("../original_data/zoning_kansas_city/")) {
	unzip(zipfile = "../original_data/zoning_kansas_city.zip", 
				exdir = "../original_data/zoning_kansas_city/")
}
zoning$kcm <- st_read("../original_data/zoning_kansas_city/Other.gdb", 
											layer = "Zoning") %>% 
	mutate(
		zone_code = str_sub(LANDUSE, 0, 1),
		land_use = case_when(
			zone_code == "D" ~ "downtown",
			LANDUSE %in% c("Commercial", "Industrial") ~ "commercial",
			LANDUSE == "Residential" ~ "residential",
			TRUE ~ "other"
		)
	) %>% 
	select(-zone_code) %>% 
	rename(geometry = SHAPE) %>% 
	st_transform(4326)

# Los Angeles
download_if_needed(
	remote = "https://opendata.arcgis.com/datasets/49ad06a6b8c945debbbea865b1832ee2_0.geojson",
	local = "../original_data/zoning_los_angeles.geojson"
)
zoning$lax <- st_read("../original_data/zoning_los_angeles.geojson") %>% 
	mutate(land_use = case_when(
		ZONE_SMRY %in% c("COMMERCIAL", "INDUSTRIAL", "PARKING") ~ "commercial",
		ZONE_SMRY == "RESIDENTIAL" ~ "residential",
		ZONE_SMRY == "OPEN SPACE" ~ "public open space",
		TRUE ~ "other"
	))

# Louisville
download_if_needed(
	remote = "https://opendata.arcgis.com/datasets/901f9c3321494238ac38a01d7e96d932_19.geojson",
	local = "../original_data/zoning_louisville.geojson"
)
zoning$lou <- st_read("../original_data/zoning_louisville.geojson") %>% 
	mutate(land_use = case_when(
		ZONING_TYPE %in% c("COMMERCIAL-INDUSTRIAL", "INDUSTRIAL", "OFFICE") ~ 
			"commercial",
		ZONING_TYPE == "RESIDENTIAL" ~ "residential",
		TRUE ~ "other"
	))

# New York
download_if_needed(
	remote = "https://www1.nyc.gov/assets/planning/download/zip/data-maps/open-data/nycgiszoningfeatures_201810shp.zip",
	local = "../original_data/zoning_new_york.zip"
)
if (!dir.exists("../original_data/zoning_new_york/")) {
	unzip(zipfile = "../original_data/zoning_new_york.zip", 
				exdir = "../original_data/zoning_new_york/")
}
zoning$nyc <- st_read("../original_data/zoning_new_york/nyzd.shp") %>% 
	mutate(
		zone_code = str_sub(ZONEDIST, 0, 1),
		land_use = case_when(
			ZONEDIST == "BALL FIELD" ~ "other",
			ZONEDIST %in% c("PARK", "PLAYGROUND", "PUBLIC PLACE") ~ 
				"public open space",
			zone_code %in% c("C", "M") ~ "commercial",
			zone_code == "R" ~ "residential",
			TRUE ~ "other"
		)
	) %>% 
	select(-zone_code) %>% 
	st_transform(4326)

# San Francisco
download_if_needed(
	remote = "https://data.sfgov.org/api/geospatial/xvjh-uu28?method=export&format=GeoJSON",
	local = "../original_data/zoning_san_francisco.geojson"
)
zoning$sfo <- st_read("../original_data/zoning_san_francisco.geojson") %>% 
	mutate(land_use = case_when(
		zoning %in% c("C-2", "M-1", "M-2", "MB-O", "MB-RA", "MUO", "PM-CF", "SALI", 
									"SLI", "SSO", "WMUO") ~ "commercial",
		str_sub(zoning, 0, 2) %in% c("NC", "PD", "RC") ~ "commercial",
		zoning %in% c("C-3-G", "C-3-O", "C-3-O(SD)", "C-3-R", "C-3-S", "CCB", 
									"CVR") ~ "downtown",
		zoning %in% c("P", "PM-OS", "TI-OS", "YBI-OS") ~ "public open space",
		zoning %in% c("CRNC", "MUR", "PM-R", "PM-S", "RED", "RED-MX", "RH DTR", 
									"SB-DTR", "TB DTR", "TI-R", "YBI-R") ~ "residential",
		str_sub(zoning, 0, 2) %in% c("RH", "RM", "RT") ~ "residential",
		TRUE ~ "other"
	))

# Tucson
download_if_needed(
	remote = "https://opendata.arcgis.com/datasets/489d09483d36427abd9738ee7d2a1de8_19.geojson",
	local = "../original_data/zoning_tucson.geojson"
)
zoning$tus <- st_read("../original_data/zoning_tucson.geojson") %>% 
	mutate(
		zone_code = str_extract(ORIG_ZONE, "^\\w+"),
		land_use = case_when(
			zone_code %in% c("C", "CB", "CI", "I", "NC", "O", "P", "RVC") ~ "commercial",
			zone_code %in% c("IR", "OS") ~ "public open space",
			zone_code %in% c("CR", "MH", "R", "RV", "RX", "SH", "SR") ~ "residential",
			TRUE ~ "other"
		)
	) %>% 
	select(-zone_code)

# Virginia Beach
download_if_needed(
	remote = "http://gis.data.vbgov.com/datasets/a3558a2790384bd0950cbdbe69e29da8_2.geojson",
	local = "../original_data/zoning_virginia_beach.geojson"
)
zoning$vib <- st_read("../original_data/zoning_virginia_beach.geojson") %>% 
	mutate(
		zone_code = str_sub(ZONING, 0, 1),
		land_use = case_when(
			ZONING %in% c("RT1", "RT3", "RT4") ~ "commercial",
			ZONING %in% c("B3", "CBC") ~ "downtown",
			ZONING == "P1" ~ "public open space",
			ZONING %in% c("A12", "A18", "A24", "A36") ~ "residential",
			zone_code %in% c("B", "I", "O") ~ "commercial",
			zone_code == "R" ~ "residential",
			TRUE ~ "other"
		)
	) %>% 
	select(-zone_code)

# union polygons
zoning <- map(zoning, function (x) {
	x %>% 
		filter(st_is_valid(x)) %>% 
		group_by(land_use) %>% 
		summarise()
})

# merge and save data
zoning %>% 
	reduce(rbind) %>% 
	rename(geometry = geom) %>% 
	rmapshaper::ms_simplify(keep = 0.1) %>% 
	filter(st_is_valid(.)) %>% 
	st_write("../analysis_data/zoning.gpkg")

rm(zoning)
```

Using the zoning data we can calculate the proportion of each blockgroup that
is zoned as either residential or commercial, and from that calculate the ratio
between the two.

```{r}
zoning <- st_read("../analysis_data/zoning.gpkg")

# calculate proportion of each blockgroup that has commercial zoning
blockgroup_outlines <- blockgroup_outlines %>% 
	select(-prop_overlap) %>% 
	add_overlap_area(filter(zoning, 
													land_use %in% c("commercial", "downtown"))) %>% 
	rename(prop_zoned_commercial = prop_overlap)
```



## Facility data

We can obtain details of retail facilities from OSM, but note that: 

  * some may be stored as polygons and some as points,
  * some may be coded as building=retail and some as shop=*

So it is necessary to do multiple queries.

```{r}
cities %>% 
	slice(2) %>% 
	apply(1, function (x) {
		
		# get data on retail buildings
		retail <- opq(bbox = paste0(x['name'], ", ", x['state'])) %>% 
			add_osm_feature(key = "building", value = "retail", 
											match_case = FALSE) %>% 
			osmdata_sf()
		
		# get data on shops
		shops <- opq(bbox = paste0(x['name'], ", ", x['state'])) %>% 
			add_osm_feature(key = "shop", match_case = FALSE) %>% 
			osmdata_sf()

})
```



# Crime data

We need five years of crime data from the CODE database. Only some columns from 
the core data are needed, but are otherwise in the correct format already so can 
be immediately passed through to a single CSV output file.

```{r}
if (!file.exists("../original_data/crime_data.csv.gz")) {
	
	crime_data <- dir("../original_data", pattern = "^crime_open_database_core_", 
										full.names = TRUE) %>% 
		# explicit column typing is required because some columns are mostly digits
		# but have a few rows with characters and so they aren't caught by the
		# column-typing algorithm
		map_df(read_csv, col_types = cols(
			.default = col_character(),
			uid = col_integer(),
			date_single = col_datetime(format = ""),
			date_start = col_datetime(format = ""),
			date_end = col_datetime(format = ""),
			multiple_dates = col_logical(),
			longitude = col_double(),
			latitude = col_double(),
			fips_state = col_integer(),
			block_group = col_integer(),
			block = col_integer()
		)) %>% 
		select(
			uid, 
			city_name, 
			offense_code, 
			offense_type, 
			offense_group, 
			offense_against, 
			date_single, 
			longitude, 
			latitude, 
			state = fips_state,
			county = fips_county, 
			census_tract = tract, 
			census_blockgroup = block_group,
			census_block = block,
			location_type,
			location_category
		)
	
	# store a copy of the crime data
	crime_data %>% 
		write_csv("../original_data/crime_data.csv.gz", na = "")
	
} else {
	
	crime_data <- read_csv("../original_data/crime_data.csv.gz")
	
}

# add whether offence occurred (approximately) during school hours
crime_data <- crime_data %>% mutate(
	school_time = ifelse(hour(date_single) >= 8 & hour(date_single) <= 17, 
											 TRUE, FALSE)
) %>% 
	# check that no columns except location_type and location_category contain NA
	# values
	assert(not_na, everything(), -location_type, -location_category) %>% 
	glimpse()
```


## Identify term-time offences

We need to know the dates on which schools were closed in each city in each 
year. Data are from the National Council on Teacher Quality [Teacher Contracts 
Database](https://www.nctq.org/contract-database/).

```{r}
school_holidays <- list(
	"Chicago" = c(
		# 2012
		# day_seq("2012-01-01", "2012-01-06"), "2012-01-16", "2012-01-27", 
		# "2012-02-03", "2012-02-13", "2012-02-20", "2012-03-05", 
		# day_seq("2012-04-02", "2012-04-06"), "2012-04-13", "2012-04-28", 
		# "2012-05-28", day_seq("2012-06-14", "2012-09-03"), "2012-10-12", 
		# "2012-11-02", "2012-11-12", "2012-11-22", "2012-11-23", 
		# day_seq("2012-12-24", "2012-12-31"),
		# 2013
		day_seq("2013-01-01", "2013-01-04"), "2013-01-21", "2013-01-25", 
		"2013-02-12", "2013-02-18", "2013-03-29", 
		day_seq("2013-04-01", "2013-04-05"), "2013-05-27",
		day_seq("2013-06-17", "2013-08-23"), "2013-09-02", "2013-10-14", 
		"2013-11-01", "2013-11-11", "2013-11-12", 
		day_seq("2013-11-27", "2013-11-29"), day_seq("2013-12-23", "2013-12-31"),
		# 2014
		day_seq("2014-01-01", "2014-01-03"), "2014-01-20", "2014-01-24",
		"2014-02-12", "2014-03-28", "2014-04-07", 
		day_seq("2014-04-14", "2014-04-18"), "2014-05-26",
		day_seq("2014-06-11", "2014-09-01"), "2014-10-13", "2014-11-07", 
		"2014-11-11", day_seq("2014-11-26", "2014-11-28"), 
		day_seq("2014-12-22", "2014-12-31"),
		# 2015
		"2015-01-01", "2015-01-02", "2015-01-19", "2015-01-30", "2015-02-16", 
		day_seq("2015-04-3", "2015-04-10"), "2015-05-25", 
		day_seq("2015-06-17", "2015-09-07"), "2015-10-12", "2015-11-11", 
		"2015-11-13", day_seq("2015-11-25", "2015-11-27"), 
		day_seq("2015-12-21", "2015-12-31"),
		# 2016
		"2016-01-01", "2016-01-18", "2016-02-05", "2016-02-15", "2016-04-08",
		day_seq("2016-04-18", "2016-04-22"), "2016-05-30",
		day_seq("2016-06-22", "2016-09-05"), "2016-10-10", "2016-11-04", 
		"2016-11-11", day_seq("2016-11-23", "2016-11-25"), 
		day_seq("2016-12-26", "2016-12-31"),
		# 2017
		day_seq("2017-01-01", "2017-01-06"), "2017-01-16", "2017-02-03",
		"2017-02-20", day_seq("2017-04-07", "2017-04-14"), "2017-05-29",
		day_seq("2017-06-21", "2017-08-31"), "2017-09-01", "2017-09-04", 
		"2017-10-09", "2017-11-03", day_seq("2017-11-22", "2017-11-24"), 
		day_seq("2017-12-25", "2017-12-31")
	),
	"Detroit" = c(
		# 2012
		# day_seq("2012-01-01", "2012-01-06"), "2012-01-16", 
		# day_seq("2012-01-20", "2012-01-24"), "2012-03-06", "2012-03-07", 
		# "2012-03-20", "2012-03-21", day_seq("2012-04-06", "2012-04-13"), 
		# "2012-05-28", day_seq("2012-06-15", "2012-09-03"), "2012-11-06", 
		# "2012-11-22", "2012-11-23", day_seq("2012-12-24", "2012-12-31"),
		# 2013
		day_seq("2013-01-01", "2013-01-04"), "2013-01-21", 
		day_seq("2013-02-18", "2013-02-22"), "2013-03-05", "2013-03-06", 
		"2013-03-19", "2013-03-20", day_seq("2013-03-29", "2013-04-05"),
		"2013-05-27", day_seq("2013-06-15", "2013-09-02"), "2013-11-05",
		"2013-11-28", "2013-11-29", day_seq("2013-12-23", "2013-12-31"),
		# 2014
		day_seq("2014-01-01", "2014-01-03"), "2014-01-20", 
		day_seq("2014-02-17", "2014-02-21"), "2014-03-04", "2014-03-05", 
		"2014-03-18", "2014-03-19", day_seq("2014-04-18", "2014-04-25"), 
		"2014-05-26", day_seq("2014-06-14", "2014-09-01"), "2014-11-04", 
		"2014-11-27", "2014-11-28", day_seq("2014-12-22", "2014-12-31"),
		# 2015
		"2015-01-01", "2015-01-02", "2015-01-19",  
		day_seq("2015-02-16", "2015-02-20"), "2015-03-03", "2015-03-04", 
		"2015-03-17", "2015-03-18", day_seq("2015-04-03", "2015-04-10"), 
		"2015-05-25", day_seq("2015-06-12", "2015-09-07"), 
		"2015-11-03", "2015-11-26", "2015-11-27", 
		day_seq("2015-12-21", "2015-12-31"),
		# 2016
		"2016-01-01", "2016-01-18", day_seq("2016-02-15", "2016-02-19"), 
		day_seq("2016-03-25", "2016-04-01"), "2016-04-12", "2016-04-13", 
		"2016-04-26", "2016-05-30", day_seq("2016-06-18", "2016-09-05"), 
		"2016-11-08", day_seq("2016-11-23", "2016-11-25"), 
		day_seq("2016-12-26", "2016-12-31"),
		# 2017
		day_seq("2017-01-01", "2017-01-06"), "2017-01-16", "2017-02-17",
		"2017-02-20", day_seq("2017-04-14", "2017-04-21"), "2017-05-29",
		day_seq("2017-06-21", "2017-09-04"), "2017-11-07", 
		day_seq("2017-11-22", "2017-11-24"), day_seq("2017-12-25", "2017-12-31")
	),
	"Fort Worth" = c(
		# 2012
		# day_seq("2012-01-01", "2012-01-02"), "2012-01-13", "2012-01-16", 
		# "2012-02-03", day_seq("2012-03-12", "2012-03-16"), "2012-04-06", 
		# "2012-05-28", day_seq("2012-06-02", "2012-08-26"), "2012-09-03", 
		# "2012-09-14", "2012-10-08", "2012-11-05", 
		# day_seq("2012-11-19", "2012-11-23"),
		# day_seq("2012-12-24", "2012-12-31"),
		# 2013
		day_seq("2013-01-01", "2013-01-07"), "2013-01-18", "2013-01-21", 
		"2013-02-01", day_seq("2013-03-11", "2013-03-15"), "2013-03-29", 
		"2013-05-27", day_seq("2013-06-08", "2013-08-25"), "2013-09-02", 
		"2013-10-14", "2013-11-11", day_seq("2013-11-25", "2013-11-29"), 
		day_seq("2013-12-23", "2013-12-31"), 
		# 2014
		day_seq("2014-01-01", "2014-01-03"), "2014-01-17", "2014-01-20", 
		"2014-02-07", day_seq("2014-03-10", "2014-03-14"), "2014-04-18", 
		"2014-05-26", day_seq("2014-06-07", "2014-08-22"), "2014-09-01", 
		"2014-10-13", "2014-11-10", day_seq("2014-11-24", "2014-11-28"), 
		day_seq("2014-12-22", "2014-12-31"),
		# 2015
		day_seq("2015-01-01", "2015-01-02"), "2015-01-16", "2015-01-19", 
		"2015-02-06", day_seq("2015-03-09", "2015-03-13"), "2015-04-03", 
		"2015-05-25", day_seq("2015-06-08", "2015-08-21"), "2015-09-07", 
		"2015-10-12", day_seq("2015-11-23", "2015-11-27"), 
		day_seq("2015-12-21", "2015-12-31"),
		# 2016
		day_seq("2016-01-01", "2016-01-04"), "2016-01-18", "2016-02-05",
		day_seq("2016-03-14", "2016-03-18"), "2016-03-25", "2016-05-30",
		day_seq("2016-06-03", "2016-08-19"), "2016-09-05", "2016-10-10",
		day_seq("2016-11-21", "2016-11-25"), day_seq("2016-12-23", "2016-12-31"),
		# 2017
		day_seq("2017-01-01", "2017-01-09"), "2017-01-16", "2017-02-03",
		day_seq("2017-03-13", "2017-03-17"), "2017-04-14", "2017-05-29",
		day_seq("2017-06-05", "2017-08-18"), "2017-09-04", "2017-10-09",
		day_seq("2017-11-20", "2017-11-24"), day_seq("2017-12-22", "2017-12-31")
	),
	"Kansas City" = c(
		# 2013
		day_seq("2013-01-01", "2013-01-07"), "2013-01-21", "2013-02-18",
		"2013-02-28", "2013-03-01", day_seq("2013-03-25", "2013-03-29"),
		day_seq("2013-05-21", "2013-08-11"), "2013-09-02", 
		day_seq("2013-10-17", "2013-10-18"), day_seq("2013-11-27", "2013-11-29"),
		day_seq("2013-12-23", "2013-12-31"),
		# 2014
		day_seq("2014-01-01", "2014-01-06"), "2014-01-20", "2014-02-06", 
		"2014-02-07", "2014-02-17", day_seq("2014-03-17", "2014-03-21"), 
		"2014-04-18", day_seq("2014-05-21", "2014-08-08"), "2014-09-01", 
		"2014-09-26", day_seq("2014-10-16", "2014-10-17"), 
		day_seq("2014-11-26", "2014-11-28"), day_seq("2014-12-22", "2014-12-31"),
		# 2015
		day_seq("2015-01-01", "2015-01-02"), "2015-01-19", 
		day_seq("2015-02-12", "2015-02-16"), day_seq("2015-03-16", "2015-03-20"),
		"2015-04-03", day_seq("2015-05-20", "2015-08-07"), "2015-09-07",
		day_seq("2015-10-15", "2015-10-16"), day_seq("2015-11-25", "2015-11-27"),
		day_seq("2015-12-21", "2015-12-31"), 
		# 2016
		"2016-01-01", "2016-01-18", day_seq("2016-02-11", "2016-02-15"),
		day_seq("2016-03-14", "2016-03-18"), "2016-03-25", 
		day_seq("2016-05-18", "2016-08-14"), day_seq("2016-09-02", "2016-09-05"), 
		"2016-10-07", "2016-10-20", "2016-10-21", 
		day_seq("2016-11-23", "2016-11-25"), day_seq("2016-12-19", "2016-12-31"),
		# 2017
		day_seq("2017-01-01", "2017-01-03"), "2017-01-16",
		day_seq("2017-02-16", "2017-02-20"), day_seq("2017-03-13", "2017-03-17"),
		"2017-04-14", day_seq("2017-05-29", "2017-08-11"), "2017-09-01",
		"2017-09-04", "2017-10-19", "2017-10-20", 
		day_seq("2017-11-22", "2017-11-24"), day_seq("2017-12-22", "2017-12-31")
	),
	"Los Angeles" = c(
		# 2012
		# day_seq("2012-01-01", "2012-01-06"), "2012-01-16", "2012-02-20", 
		# day_seq("2012-03-30", "2012-04-06"), "2012-05-28", 
		# day_seq("2012-06-25", "2012-08-13"), day_seq("2012-08-31", "2012-09-03"),
		# "2012-09-17", "2012-09-26", "2012-11-12", 
		# day_seq("2012-11-22", "2012-11-23"), day_seq("2012-12-17", "2012-12-31"),
		# 2013
		day_seq("2013-01-01", "2013-01-04"), "2013-02-21", "2013-02-18", 
		day_seq("2013-03-25", "2013-04-01"), "2013-05-27", 
		day_seq("2013-06-05", "2013-08-12"), day_seq("2013-08-30", "2013-09-02"),
		"2013-09-05", "2013-11-11", day_seq("2013-11-25", "2013-11-29"),
		day_seq("2013-12-23", "2013-12-31"),
		# 2014
		day_seq("2014-01-01", "2014-01-10"), "2014-01-20", "2014-02-17",
		"2014-03-31", day_seq("2014-04-14", "2014-04-18"), "2014-05-26",
		day_seq("2014-06-06", "2014-08-11"), day_seq("2014-08-29", "2014-09-01"),
		"2014-09-25", "2014-11-11", day_seq("2014-11-24", "2014-11-28"),
		day_seq("2014-12-22", "2014-12-31"),
		# 2015
		day_seq("2015-01-01", "2015-01-09"), "2015-01-19", "2015-02-16",
		day_seq("2015-03-30", "2015-04-06"), "2015-05-25", 
		day_seq("2015-06-05", "2015-08-17"), day_seq("2015-09-04", "2015-09-07"),
		"2015-09-14", "2015-09-23", "2015-11-11", 
		day_seq("2015-11-23", "2015-11-27"), day_seq("2015-12-21", "2015-12-31"),
		# 2016
		day_seq("2016-01-01", "2016-01-08"), "2016-01-18", "2016-01-15", 
		day_seq("2016-03-21", "2016-03-28"), "2016-05-30", 
		day_seq("2016-06-13", "2016-08-15"), day_seq("2016-09-02", "2016-09-05"),
		"2016-10-03", "2016-10-12", "2016-11-11", 
		day_seq("2016-11-21", "2016-11-25"), day_seq("2016-12-19", "2016-12-31"),
		# 2017
		day_seq("2017-01-01", "2017-01-06"), "2017-01-16", "2017-02-20",
		"2017-03-31", day_seq("2017-04-10", "2017-04-14"), "2017-05-29",
		day_seq("2017-06-12", "2017-08-14"), "2017-09-01", "2017-09-04",
		"2017-09-21", "2017-11-10", day_seq("2017-11-20", "2017-11-24"),
		day_seq("2017-12-18", "2017-12-31")
	),
	"Louisville" = c(
		# 2012
		# day_seq("2012-01-01", "2012-01-02"), "2012-01-16", "2012-02-13", 
		# "2012-03-02", day_seq("2012-04-09", "2012-04-13"), "2012-05-04",
		# "2012-05-22", day_seq("2012-05-28", "2012-08-20"), "2012-09-03",
		# "2012-10-05", "2012-10-08", "2012-11-05", "2012-11-06", 
		# day_seq("2012-11-21", "2012-11-23"), day_seq("2012-12-21", "2012-12-31"),
		# 2013
		day_seq("2013-01-01", "2013-01-04"), "2013-01-21", 
		day_seq("2013-02-25", "2013-03-01"), day_seq("2013-04-01", "2013-04-05"),
		"2013-05-03", "2013-05-27", day_seq("2013-06-06", "2013-08-19"),
		"2013-09-02", day_seq("2013-10-04", "2013-10-08"), "2013-11-11",
		day_seq("2013-11-27", "2013-11-29"), day_seq("2013-12-23", "2013-12-31"),
		# 2014
		day_seq("2014-01-01", "2014-01-03"), "2014-01-20", 
		day_seq("2014-02-24", "2014-02-28"), day_seq("2014-03-31", "2014-04-04"),
		"2014-05-02", "2014-05-20", "2014-05-26", 
		day_seq("2014-06-05", "2014-08-12"), "2014-09-01", "2014-10-03", 
		"2014-10-06", "2014-10-07", "2014-11-03", "2014-11-04", 
		day_seq("2014-11-26", "2014-11-28"), day_seq("2014-12-22", "2014-12-31"),
		# 2015
		day_seq("2015-01-01", "2015-01-02"), "2015-01-19", "2015-02-27",
		"2015-03-09", day_seq("2015-04-02", "2015-04-10"), "2015-05-01",
		"2015-05-19", "2015-05-25", day_seq("2015-05-29", "2015-08-11"),
		"2015-09-07", day_seq("2015-10-02", "2015-10-06"), "2015-11-02", 
		"2015-11-03", day_seq("2015-11-25", "2015-11-27"),
		day_seq("2015-12-21", "2015-12-31"), 
		# 2016
		"2016-01-01", "2016-01-18", "2016-02-29", "2016-03-14", 
		day_seq("2016-03-31", "2016-04-08"), "2016-05-06", "2016-05-17",
		day_seq("2016-05-26", "2016-08-09"), "2016-09-05", 
		day_seq("2016-09-30", "2016-10-04"), "2016-11-07", "2016-11-08",
		day_seq("2016-11-23", "2016-11-25"), day_seq("2016-12-19", "2016-12-31"),
		# 2017
		day_seq("2017-01-01", "2017-01-02"), "2017-01-16", "2017-02-27",
		"2017-03-13", day_seq("2017-03-30", "2017-04-07"), "2017-05-05",
		day_seq("2017-05-25", "2017-08-15"), "2017-09-04", 
		day_seq("2017-10-06", "2017-10-10"), day_seq("2017-11-22", "2017-11-24"),
		day_seq("2017-12-20", "2017-12-31")
	),
	"New York" = c(
		# 2012
		# day_seq("2012-01-01", "2012-01-02"), "2012-01-16", "2012-01-30",
		# day_seq("2012-02-20", "2012-02-24"), day_seq("2012-04-06", "2012-04-13"),
		# "2012-05-28", "2012-06-07", "2012-06-22", 
		# day_seq("2012-06-28", "2012-09-05"), "2012-09-17", "2012-09-18", 
		# "2012-09-26", "2012-10-08", "2012-11-06", "2012-11-12", "2012-11-22", 
		# "2012-11-23", day_seq("2012-12-24", "2012-12-31"),
		# 2013
		"2013-01-01", "2013-01-21", "2013-01-28", 
		day_seq("2013-02-18", "2013-02-22"), day_seq("2013-03-25", "2013-04-02"),
		"2013-05-27", "2013-06-06", "2013-06-21", 
		day_seq("2013-06-26", "2013-09-08"), "2013-10-14", "2013-11-05", 
		"2013-11-11", "2013-11-28", "2013-11-29", 
		day_seq("2013-12-23", "2013-12-31"),
		# 2014
		"2014-01-01", "2014-01-20", "2014-01-31", 
		day_seq("2014-02-17", "2014-02-21"), day_seq("2014-04-14", "2014-04-22"),
		"2014-05-26", "2014-06-05", day_seq("2014-06-27", "2014-09-03"),
		"2014-09-25", "2014-09-26", "2014-10-13", "2014-11-04", "2014-11-11",
		"2014-11-27", "2014-11-28", day_seq("2014-12-24", "2014-12-31"),
		# 2015
		day_seq("2015-01-01", "2015-01-04"), "2015-01-19", "2015-01-30", 
		"2015-02-02", day_seq("2015-02-16", "2015-02-20"), 
		day_seq("2015-04-03", "2015-04-10"), "2015-05-25", "2015-06-04", 
		"2015-06-25", day_seq("2015-06-27", "2015-09-08"), "2015-09-14", 
		"2015-09-15", "2015-09-23", "2015-09-24", "2015-10-12", "2015-11-03", 
		"2015-11-11", "2015-11-26", "2015-11-27", 
		day_seq("2015-12-24", "2015-12-31"),
		# 2016
		"2016-01-01", "2016-01-18", "2016-02-01", 
		day_seq("2016-02-15", "2016-02-19"), "2016-03-25", 
		day_seq("2016-04-25", "2016-04-29"), "2016-05-30", "2016-06-09",
		"2016-06-23", day_seq("2016-06-29", "2016-09-07"), "2016-09-12",
		"2016-10-03", "2016-10-04", "2016-10-10", "2016-10-12", "2016-11-08",
		"2016-11-11", "2016-11-24", "2016-11-25", 
		day_seq("2016-12-26", "2016-12-31"),
		# 2017
		day_seq("2017-01-01", "2017-01-02"), "2017-01-16", "2017-01-30",
		day_seq("2017-02-20", "2017-02-24"), day_seq("2017-04-10", "2017-04-18"),
		"2017-05-29", "2017-06-08", "2017-06-23", "2017-06-26",
		day_seq("2017-06-29", "2017-09-06"), "2017-09-21", "2017-09-22",
		"2017-10-09", "2017-11-07", "2017-11-23", "2017-11-24",
		day_seq("2017-12-25", "2017-12-31")
	),
	"San Francisco" = c(
		# 2012
		# day_seq("2012-01-01", "2012-01-02"), "2012-01-16", "2012-01-23", 
		# "2012-02-17", "2012-02-20", "2012-03-16", 
		# day_seq("2012-03-26", "2012-03-30"), "2012-04-06", "2012-04-23",
		# day_seq("2012-05-28", "2012-08-17"), "2012-09-03", "2012-10-08",
		# "2012-11-12", day_seq("2012-11-21", "2012-11-23"), 
		# day_seq("2012-12-24", "2012-12-31"),
		# 2013
		day_seq("2013-01-01", "2013-01-04"), "2013-01-21", "2013-02-11",
		"2013-02-18", day_seq("2013-03-25", "2013-03-29"), "2013-05-27",
		day_seq("2013-06-03", "2013-08-16"), "2013-09-02", "2013-10-14",
		"2013-11-11", day_seq("2013-11-27", "2013-11-29"), 
		day_seq("2013-12-23", "2013-12-31"),
		# 2014
		day_seq("2014-01-01", "2014-01-03"), "2014-01-20", "2014-01-31",
		"2014-02-17", day_seq("2014-03-31", "2014-04-04"), "2014-05-26",
		day_seq("2014-06-02", "2014-08-15"), "2014-09-01", "2014-10-13",
		"2014-11-11", day_seq("2014-11-26", "2014-11-28"),
		day_seq("2014-12-22", "2014-12-31"),
		# 2015
		day_seq("2015-01-01", "2015-01-02"), "2015-01-19", 
		day_seq("2015-02-19", "2015-02-20"), day_seq("2015-03-30", "2015-04-03"),
		"2015-05-25", day_seq("2015-06-01", "2015-08-14"), "2015-09-07",
		"2015-10-12", "2015-11-11", day_seq("2015-11-25", "2015-11-27"),
		day_seq("2015-12-21", "2015-12-31"), 
		# 2016
		"2016-01-01", "2016-01-18", "2016-02-08", "2016-02-15", 
		day_seq("2016-03-28", "2016-04-01"), day_seq("2016-05-27", "2016-08-12"),
		"2016-09-05", "2016-10-10", "2016-11-11", 
		day_seq("2016-11-23", "2016-11-25"), day_seq("2016-12-19", "2016-12-31"),
		# 2017
		day_seq("2017-01-01", "2017-01-02"), "2017-01-16", "2017-01-27", 
		"2017-02-20", day_seq("2017-03-27", "2017-03-31"), 
		day_seq("2017-05-29", "2017-08-18"), "2017-09-04", "2017-10-09",
		"2017-11-10", day_seq("2017-11-20", "2017-11-24"), 
		day_seq("2017-12-22", "2017-12-31")
	),
	"Tucson" = c(
		# 2013 from http://edweb.tusd1.org/sahuaro/2012_2013/SCHOOL_CALENDAR-English_2012-2013_pdf.pdf
		# and https://www.dhaydock.org/2013-2014/2013-14%20CALENDAR.pdf
		day_seq("2013-01-01", "2013-01-04"), "2013-01-21", "2013-02-07", 
		"2013-02-08", day_seq("2013-02-21", "2013-02-26"), "2013-03-15",
		day_seq("2013-03-25", "2013-04-01"),  day_seq("2013-05-24", "2013-08-09"), 
		"2013-09-03", "2013-10-21", "2013-11-11", 
		day_seq("2013-11-25", "2013-11-29"), day_seq("2013-12-23", "2013-12-31"),
		# 2014
		day_seq("2014-01-01", "2014-01-03"), "2014-01-20", "2014-02-10",
		"2014-02-17", "2014-03-03", day_seq("2014-04-18", "2014-04-25"),
		"2014-05-02", "2015-05-26", day_seq("2014-06-02", "2014-07-31"),
		"2014-09-01", day_seq("2014-09-03", "2014-09-10"), "2014-11-11",
		"2014-11-27", "2014-11-28", day_seq("2014-12-19", "2014-12-31"),
		# 2015
		day_seq("2015-01-01", "2015-01-02"), "2015-01-19", "2015-02-26",
		"2015-02-27", day_seq("2015-03-13", "2015-03-20"), "2015-04-03",
		day_seq("2015-05-22", "2015-08-06"), "2015-09-07", 
		day_seq("2015-10-09", "2015-10-16"), "2015-11-11", "2015-11-26", 
		"2015-11-27", day_seq("2015-12-18", "2015-12-31"),
		# 2016
		"2016-01-01", "2016-01-18", "2016-02-25", "2016-02-26", 
		day_seq("2016-03-18", "2016-03-25"), day_seq("2016-05-26", "2016-08-03"),
		"2016-09-05", day_seq("2016-10-07", "2016-10-14"), "2016-11-11",
		"2016-11-24", "2016-11-25", day_seq("2016-12-23", "2016-12-31"),
		# 2017
		day_seq("2017-01-01", "2017-01-06"), "2017-01-16", "2017-02-23",
		"2017-02-24", day_seq("2017-03-17", "2017-03-24"), "2017-04-14",
		day_seq("2017-05-26", "2017-08-03"), "2017-09-04",
		day_seq("2017-10-06", "2017-10-13"), "2017-11-10", "2017-11-23",
		"2017-11-24", day_seq("2017-12-22", "2017-12-31")
	),
	"Virginia Beach" = c(
		# 2013
		"2013-01-01", "2013-01-21", "2013-01-25", "2013-02-18", 
		day_seq("2013-03-29", "2013-04-05"), "2013-05-27", 
		day_seq("2013-06-17", "2013-09-02"), "2013-11-05", "2013-11-11",
		"2013-11-28", "2013-11-29", day_seq("2013-12-23", "2013-12-31"),
		# 2014
		"2014-01-01", "2014-01-20", "2014-01-24", "2014-03-28", 
		day_seq("2014-04-14", "2014-04-18"), "2014-05-26",
		day_seq("2014-06-16", "2014-09-01"), "2014-11-04", "2014-11-11",
		"2014-11-27", "2014-11-28", day_seq("2014-12-22", "2014-12-31"),
		# 2015
		"2015-01-01", "2015-01-02", "2015-01-19", "2015-01-26", "2015-01-27",
		"2015-02-16", day_seq("2015-04-03", "2015-04-10"), "2015-05-25", 
		day_seq("2015-06-19", "2015-09-07"), "2015-11-03", "2015-11-11", 
		"2015-11-26", "2015-11-27", day_seq("2015-12-24", "2015-12-31"),
		# 2016
		"2016-01-01", "2016-01-18", "2016-02-01", "2016-02-15", 
		day_seq("2016-03-28", "2016-04-01"), "2016-04-13", "2016-05-30",
		day_seq("2016-06-20", "2016-09-05"), "2016-10-10", "2016-10-24",
		"2016-11-08", "2016-11-11", "2016-11-24", "2016-11-25",
		day_seq("2016-12-23", "2016-12-31"),
		# 2017
		day_seq("2017-01-01", "2017-01-02"), "2017-01-16", "2017-01-30",
		"2017-02-20", "2017-03-08", day_seq("2017-04-10", "2017-04-14"),
		"2017-05-29", day_seq("2017-06-19", "2017-09-04"), "2017-10-09",
		"2017-11-07", "2017-11-23", "2017-11-24", 
		day_seq("2017-12-21", "2017-12-31")
	)
)

school_holidays <- map_df(names(school_holidays), function (x) {
	
	these_holidays <- ymd(school_holidays[[x]])

	tibble(
		city = x,
		date = seq.Date(
			ymd(paste(min(year(these_holidays)), "01", "01", sep = "-")),
			ymd(paste(max(year(these_holidays)), "12", "31", sep = "-")),
			by = "days"
		),
		weekday = wday(date, label = TRUE)
	) %>% 
		mutate(school_day = if_else(
			date %in% these_holidays | weekday %in% c("Sat", "Sun"), 
			FALSE, TRUE))

	}) %>% 
	arrange(date, city) %>% 
	# check that all dates are within the expected range
	verify(date >= ymd("2013-01-01") & date <= ymd("2017-12-31"))

# show table of count of school days each year
school_holidays %>% 
	mutate(year = year(date)) %>% 
	mutate(year = ifelse(
		month(date) <= 7, 
		paste(year - 1, str_sub(year, 3), sep = "-"), 
		paste(year, str_sub(year + 1, 3), sep = "-")
	)) %>% 
	group_by(city, year) %>% 
	summarise(school_days = sum(school_day)) %>% 
	spread(city, school_days)

# show chart of holidays each year
school_holidays %>% 
	mutate(
		day_of_year = yday(date),
		school_day = case_when(
			weekday %in% c("Sat", "Sun") ~ "weekend",
			school_day == TRUE ~ "school day",
			TRUE ~ "vacation day"
		),
		year = year(date)
	) %>% 
	ggplot(aes(x = day_of_year, y = city, fill = school_day)) +
	geom_tile() +
	scale_x_continuous(expand = c(0, 0), breaks = seq(0, 366, by = 30)) +
	scale_fill_manual(values = c("school day" = "#CCCCCC", "vacation day" = "#666666", 
															 "weekend" = "#999999")) +
	labs(x = NULL, y = NULL) +
	facet_grid(rows = vars(year)) +
	theme_minimal() +
	theme(
		axis.ticks.x = element_line(),
		legend.position = "bottom",
		legend.title = element_blank(),
		panel.grid = element_blank()
	)

crime_data <- crime_data %>% 
	mutate(date_single = as.Date(date_single)) %>% 
	left_join(school_holidays, 
						by = c("city_name" = "city", "date_single" = "date")) %>% 
	mutate(
		school_day = ifelse(weekday %in% c("Sat", "Sun"), NA, school_day),
		school_time = ifelse(weekday %in% c("Sat", "Sun"), NA, school_time)
	)

rm(school_holidays)
```


## Identify private (dwelling) vs public offenses

```{r}
crime_data <- crime_data %>% 
	mutate(public_location = case_when(
		location_category %in% c("leisure", "open space", "retail", "street",
														 "transportation") ~ "public",
		location_category == "residence" ~ "private",
		TRUE ~ "other"
	)) %>% 
	assert(in_set("public", "private", "other", allow.na = FALSE), 
				 public_location) %>% 
	glimpse()
```


## Calculate crime counts

For analysis, we need a tibble of crime counts for each blockgroup. To test the
hypotheses, we need separate counts for the various states of school days,
school times and location types.

The crime data already contain the information necessary to construct block 
group ID codes, so there is no need to carry out any spatial operations.

```{r}
crime_counts <- crime_data %>% 
	mutate(
		crime_type = as.factor(case_when(
			offense_code %in% c("13A", "13B", "13C") ~ "assault",
			offense_code %in% c("12A") ~ "robbery", # only personal robbery
			TRUE ~ NA_character_
		)),
		blockgroup_id = paste0(str_pad(state, 2, pad = "0"), county, census_tract,
													 census_blockgroup)
	) %>% 
	# remove crimes that are of types we aren't studying, as well as crimes
	# occurring at weekends (for which school_day and school_time are both set to
	# NA)
	filter(!is.na(crime_type), !is.na(school_day), !is.na(school_time)) %>% 
	group_by(blockgroup_id, crime_type, school_day, school_time, 
					 public_location) %>% 
	summarise(count = n()) %>% 
	ungroup() %>% 
	assert(not_na, crime_type, school_day, school_time, public_location) %>% 
	# complete() adds rows for blockgroups with zero counts in particular 
	# categories
	complete(blockgroup_id, crime_type, school_day, school_time, public_location, 
					 fill = list(count = 0)) %>% 
	glimpse()

# attach crime counts to block groups
# note that blockgroup_outlines must be on the left-hand side of this join,
# otherwise blocks without any offenses wouldn't be joined to anything and would
# be stripped from the data
blockgroup_data <- left_join(blockgroup_outlines, crime_counts, 
														 by = c("geoid" = "blockgroup_id")) %>% 
	mutate(count = ifelse(is.na(count), 0, count)) %>% 
	# we can remove rows relating to blockgroups that do not contain any land
	filter(area_land > 0) %>% 
	# we can remove the geometry column, since spatial operations are complete and
	# it can complicate subsetting
	st_set_geometry(NULL) %>% 
	glimpse()

# Blocks without any offenses will only appear once in blockgroup_data, and 
# the values of crime_type, school_day, school_time and public_location will be
# set to NA because there were no rows in crime_counts to join to 
# blockgroup_outlines in the join directly above.
# 1. Create a data frame of blocks with no crimes, with each block having a row
# for each value of crime_type, school_day, school_time and public_location
no_crime_blockgroups <- expand.grid(
	geoid = unique_valid(blockgroup_data$geoid[is.na(blockgroup_data$crime_type)]),
	crime_type = unique_valid(blockgroup_data$crime_type),
	school_day = unique_valid(blockgroup_data$school_day),
	school_time = unique_valid(blockgroup_data$school_time),
	public_location = unique_valid(blockgroup_data$public_location),
	stringsAsFactors = FALSE
) %>% 
	arrange(geoid)
# 2. Add all the other columns to this data frame
no_crime_blockgroups <- no_crime_blockgroups %>% 
	left_join(select(blockgroup_data, -crime_type, -school_day, -school_time, 
									 -public_location), by = "geoid") %>% 
	glimpse()
# 3. Add these rows to the original dataset, having first filtered out the rows
# in it containing NA values
blockgroup_data <- blockgroup_data %>% 
	filter(!is.na(crime_type)) %>% 
	bind_rows(no_crime_blockgroups)

# we can now remove rows for public and private location types in cities for
# which we don't have location types, to reduce the number of rows to be 
# processed later on
blockgroup_data <- blockgroup_data %>% 
	filter(!(city_name %in% c("Detroit", "Kansas City", "San Francisco", "Tucson",
														"Virginia Beach") & public_location != "other"))
	
# summarise the values of school_day and school_time in a single variable, to be
# used in multi-level modelling
blockgroup_data <- blockgroup_data %>% 
	mutate(period = case_when(
		school_day == FALSE & school_time == FALSE ~ 
			"non-school day, non-school time",
		school_day == FALSE & school_time == TRUE ~ "non-school day, school time",
		school_day == TRUE & school_time == FALSE ~ "school day, non-school time",
		school_day == TRUE & school_time == TRUE ~ "school day, school time",
		TRUE ~ NA_character_
	))

# remove unnecessary objects
rm(crime_data, crime_counts, no_crime_blockgroups)
```


## Calculate spatial lags of crime counts

To account for spatial autocorrelation in crime counts, we will calculate the 
mean count of crimes in queen-adjacent cells.

```{r}
# create a composite variable
blockgroup_data <- blockgroup_data %>% 
	mutate(selector = paste(crime_type, school_day, school_time, public_location))

# for each row, calculate the mean of the count of surrounding blockgroups
system.time(
blockgroup_data$count_lag <- map(1:nrow(blockgroup_data), function (i) {
	
	# get crime counts for neighbouring blockgroups
	# note the subset is wrapped in which() to prevent R adding rows of NA values
	# as described at https://stackoverflow.com/a/37351328/8222654
	nb_vals <- blockgroup_data$count[
		blockgroup_data$geoid %in% blockgroup_data$adj_geoid[[i]] 
		& blockgroup_data$selector == blockgroup_data$selector[[i]]
		# & blockgroup_data$crime_type == blockgroup_data$crime_type[[i]]
		# & blockgroup_data$school_day == blockgroup_data$school_day[[i]]
		# & blockgroup_data$school_time == blockgroup_data$school_time[[i]]
		# & blockgroup_data$public_location == blockgroup_data$public_location[[i]]
		]
	
	# calculate lag as the arithmetic mean of the count of crimes in surrounding
	# blockgroups
	# Note that since a cell intersects with itself, we need to account for 
	# this when calculating the mean crime count for neighbouring cells
	(sum(nb_vals) - blockgroup_data$count[[i]]) / (length(nb_vals) - 1)
	
}) %>% 
	unlist() %>% 
	replace_na(0)
)

# remove the composite variable
blockgroup_data <- blockgroup_data %>% 
	select(-selector)
```


# School data

The latest data on school locations available from the CCD is for 2015—16. 
Earlier data are available going back to 1986—87 but the format of the data 
varies from year to year. The CSV files available contain the school addresses 
but not a lat/lon or other geocode. For 2015—16 only, there is a SHP file (with 
the same information included in an Excel file) showing the location and most of 
the data about each school.

Details of the file formats are available at 
https://nces.ed.gov/ccd/pubschuniv.asp


## Public schools

The information on each school is separated into several files that can be 
merged using the NCESSCH school code. This code loads each file, selects the 
necessary variables (from the large number of variables present) and then merges 
that data with the geocoded data from the Excel file. Since spatial calculations 
can be slow, the geocoded data are also filtered to extract only those schools 
in counties that cover the CODE cities.

```{r}
# Import school data and mutate variables
school_info <- read_csv("../original_data/ccd_sch_029_1516_w_1a_011717.csv") %>% 
	select(
		school_code = NCESSCH,
		school_district = LEA_NAME,
		school_type = SCH_TYPE_TEXT,
		school_status = SY_STATUS_TEXT,
		school_level = LEVEL,
		charter_school = CHARTER_TEXT,
		virtual_school = VIRTUAL
	) %>% 
	mutate(
		school_level = case_when(
			school_level == 1 ~ "Primary",
			school_level == 2 ~ "Middle",
			school_level == 3 ~ "High",
			school_level == 4 ~ "Other",
			TRUE ~ "Not known"
		),
		charter_school = case_when(
			charter_school == "No" ~ FALSE,
			charter_school == "Yes" ~ TRUE,
			TRUE ~ NA
		),
		virtual_school = case_when(
			virtual_school == "No" ~ FALSE,
			virtual_school == "Yes" ~ TRUE,
			TRUE ~ NA
		)
	) %>% 
	glimpse()

# Import school characteristics
school_characteristics <- read_csv("../original_data/ccd_sch_129_1516_w_1a_011717.csv") %>% 
	select(
		school_code = NCESSCH,
		magnet_school = MAGNET_TEXT,
		shared_time_school = SHARED_TIME
	) %>% 
	mutate(
		magnet_school = case_when(
			magnet_school == "No" ~ FALSE,
			magnet_school == "Yes" ~ TRUE,
			TRUE ~ NA
		),
		shared_time_school = case_when(
			shared_time_school == "No" ~ FALSE,
			shared_time_school == "Yes" ~ TRUE,
			TRUE ~ NA
		)
	) %>% 
	glimpse()

# Import student numbers
school_membership <- read_csv("../original_data/ccd_sch_052_1516_w_1a_011717.csv") %>% 
	select(
		school_code = NCESSCH,
		students = TOTAL
	) %>% 
	glimpse()
```

```{r}
# Import location data and filter out schools that aren't in counties covered by 
# the CODE data (althoug CODE data originates from cities, there is no city code 
# in the schools data)
school_locations <- readxl::read_excel("../original_data/EDGE_GEOCODE_PUBLICSCH_1516.xlsx") %>% 
	# note that CNTY15 is a character variable because some state FIPS codes have
	# a leading zero
	filter(CNTY15 %in% code_counties) %>% 
	select(
		school_code = NCESSCH,
		school_name = NAME,
		county = CNTY15,
		latitude = LAT1516,
		longitude = LON1516
	) %>% 
	glimpse()

# Merge and filter the data
# The spatial data are on the left-hand side of the join because that will bring
# over from the other files only those rows relating to schools in counties
# containing CODE cities
school_data <- school_locations %>% 
	left_join(school_info, by = c("school_code")) %>% 
	left_join(school_characteristics, by = c("school_code")) %>% 
	left_join(school_membership, by = c("school_code")) %>% 
	mutate(students = ifelse(students < 0 | is.na(students), 0, students)) %>% 
	glimpse()

# remove temporary variables
rm(school_characteristics, school_info, school_locations, school_membership)
```

## Private schools

These data are from https://nces.ed.gov/surveys/pss/pssdata.asp and include 
lat/lon pairs, so we don't have to add geographic information separately

```{r}
# load private school data
private_schools <- read_csv("../original_data/pss1516_pu.csv") %>% 
	# select(-matches("^repw")) %>% 
	select(
		school_code = ppin,
		school_name = pinst,
		school_level = level,
		school_in_home = p425,
		students = numstuds,
		state = pstansi,
		county = pcnty,
		latitude = latitude16,
		longitude = longitude16,
	) %>% 
	mutate(
		school_level = case_when(
			school_level == 1 ~ "Primary",
			school_level == 2 ~ "Middle",
			school_level == 3 ~ "High",
			school_level == 4 ~ "Other",
			TRUE ~ "Not known"
		), 
		county = paste0(str_pad(state, 2, "left", "0"), county)
	) %>% 
	# filter out schools that are located in a private residence or that are not
	# in a county with a CODE city in it, or that 
	filter(school_in_home == 2 & county %in% code_counties) %>% 
	select(-state, -school_in_home) %>% 
	glimpse()

# add private school data to public school data
school_data <- bind_rows(
	list("public" = school_data, "private" = private_schools), 
	.id = "public_school"
)

# clean up
rm(private_schools)
```


## Filter by location and add census identifiers

```{r}
# convert school data to an SF object
school_data <- st_as_sf(school_data, coords = c("longitude", "latitude")) %>% 
	st_set_crs(4326)

# determine if each school is in a CODE city
school_data <- school_data %>% 
	st_join(code_city_outlines) %>% 
	filter(!is.na(city_name)) %>% 
	# we remove these variables because they also exist in blockgroup_outlines
	select(-state, -county, -city_name)

# add census identifiers to school data
school_data <- school_data %>% 
	st_join(blockgroup_outlines) %>% 
	select(-area_land, -area_water)
```


## Calculate school and student counts

At this stage we only include high schools and middle schools (i.e. 
approximately students aged 10 and over). We will exclude very-small schools 
(fewer than 10 students).

```{r}
# count schools and students in each block group
school_counts <- school_data %>% 
	st_set_geometry(NULL) %>% 
	filter(
		school_level %in% c("High", "Middle"),
		!school_status %in% c("Closed", "Future School", "Inactive"),
		students > 10
	) %>% 
	mutate(
		blockgroup_id = paste0(state, county, census_tract, census_block_group)
	) %>% 
	group_by(blockgroup_id) %>% 
	summarise(
		school_count = n(), 
		school_students = sum(students)
	) %>% 
	glimpse()

# attach school counts to block groups
blockgroup_data <- blockgroup_data %>% 
	left_join(school_counts, by = c("geoid" = "blockgroup_id")) %>% 
	mutate_at(vars("school_count", "school_students"), 
						funs(ifelse(is.na(.), 0, .))) %>% 
	mutate(school = ifelse(school_count > 0, TRUE, FALSE)) %>% 
	glimpse()

# remove unnecessary objects
rm(school_counts, school_data)
```


# Filter data

Since our focus is on the association between school location and neighbourhood
crime, we are primarily interested in residential areas. Also studying 
industrial zones, etc would substantially increase the variables that needed to
be controlled in the model. For this reason, we will exclude any blockgroup with
a population of zero. In particular, this removes LAX, O'Hare and Rikers Island,
which could otherwise be problematic.

```{r}
blockgroup_data <- blockgroup_data %>% filter(pop_total > 0)
```


# Export data and clean up

```{r}
blockgroup_data %>% 
	# remove list and geometry columns, since we have finished calculating lags
	# and running spatial operations
	select(-adj, -adj_geoid) %>% 
	# change type of some variables
	mutate_at(vars(starts_with("land_cover_prop_")), ~ifelse(. > 1, 1, .)) %>% 
	mutate_at(vars(state, census_block_group, count, school_count, 
								 school_students, pop_total), as.integer) %>% 
	mutate_at(vars(crime_type, period), as.factor) %>% 
	# write data
	write_rds("../analysis_data/blockgroup_data.Rds", compress = "gz")

#rm(blockgroup_data, blockgroup_outlines, code_city_outlines, code_counties)
```

